ğŸ’» Hand Gesture Controlled Automation (Python + ML + MediaPipe)

This project lets me control my PC using just hand gestures in front of a webcam.
No mouse. No keyboard. Just âœŒï¸ğŸ‘ŠğŸ–ï¸ and boom â€” automation.

I wanted to build something that works live and reacts instantly.
And yes â€” it actually works.

âœ‹ Gestures & Actions (Real-Time)
Gesture	Action
âœŒï¸ Victory	Opens Chrome
ğŸ‘Š Fist	Opens ChatGPT
ğŸ–ï¸ Palm	Closes Chrome

Note: Each gesture triggers an action only once. Holding it won't spam.

âš™ï¸ How It Works (Under the Hood)

Webcam captures live hand movement

MediaPipe tracks 21 hand landmarks

Coordinates are normalized (so position doesn't matter)

A RandomForest ML model predicts the gesture

Based on prediction, Python runs system-level commands (like opening Chrome)

Everything runs live using OpenCV â€”
Real-time input â†’ real-time action.

ğŸ§  What I Actually Faced

âœŒï¸ gesture was failing unless my hand was super close to camera
â†’ Fixed using wrist-based normalization

CSV files had header/index issues
â†’ Took some solid debugging to clean training data

Real-time loop was glitchy at first
â†’ Added frame wait + model output filtering

ğŸ“ Project Files (Cleaned & Functional)

realtime.py â†’ Final live system

guesture_model.pkl â†’ Trained model

extract_landmaK.py â†’ Collects normalized landmark data

evaluate.py â†’ Trains & evaluates ML model

*_landmark_norm.csv â†’ Normalized data

ğŸ› ï¸ Tech Stack

Python

MediaPipe Hands

OpenCV

Scikit-Learn

ğŸ”¥ What I Learnt

Real-time ML system building

Landmark normalization logic

Using RandomForest for classification

Practical issues in gesture control

Automating system using pure Python

And yeah â€” debugging hell and coming out alive ğŸ’€ğŸ’ª

ğŸ™‹â€â™‚ï¸ Made by
Shourya Singh
Built from scratch, tested in pain, now running smooth ğŸ˜
